练习：

1.证明：

![image-20211014225607612](decision-tree-task\images\image-20211014225607612.png)



![image-20211014225630092](C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211014225630092.png)

![image-20211014225646533](C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211014225646533.png)

![image-20211014225701395](C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211014225701395.png)

2.不会。因为决策树对于特征进行分类本质上是if-else关系，依靠的是数据间的相对大小关系（相对顺序），所以输入特征即使是经过归一化后，数据点分布的位置关系实际上并未产生改变。

3.将原来的系数由(1-γ)变为（1-γ**2），实际上是削弱了惩罚。

4.可能。采用相同参数下（如果不采取最优增益生长），深度优先生长可能会长成一个深度很深的不平衡树，而广度优先生长不会。但是如果采用最佳增益生长就不会存在这个问题，生长成的树实际上是一样的。

5.模型的目标函数就是各个叶子结点的不纯度+正则化项部分。

![image-20211014231824592](C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211014231824592.png)

6.![image-20211014225816173](C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211014225816173.png)



7.![image-20211014225858557](C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211014225858557.png)

8.决策树进行结点划分时，将每个特征值划分到结点单独一个叶节点即可。就能够实现每个点的预测。

9.min_samples_leaf 指的是叶节点内的样本数量，样本数量小的话，对样本均值的估计方差就大，即不平滑。如果样本数大的话，方差就小，输出值就平滑。



知识回顾：

1.

- ID3:取值多的属性，更容易使数据更纯，其信息增益更大。训练得到的是一颗庞大且深度浅的树：不合理。

- C4.5：采用信息增益比代替信息增益。

- CART：以基尼系数替代熵，最小化不纯度，而不是最大化信息增益。

2.结点分裂以后带来了多少不确定性的降低或者说是纯度的提高。衡量了用该特征进行分类提高纯度的能力。缺陷：对于取值数目多的特征有所偏好。

3.max_feature中随机选择特征的随机性。控制radom_split的随机性。

4.

处理连续型属性时，需要将其离散化，将连续型属性的值划分到不同的区间（类似于二叉排序树），比较各个分裂点的Gain值的大小。

根据缺失比例加上一个对应的惩罚系数。

5.![image-20211014230842063](C:\Users\86188\AppData\Roaming\Typora\typora-user-images\image-20211014230842063.png)

数学家想到的另一种用一阶泰勒展开代替信息增益公式表示纯度的方法。

GIni系数在信息熵基础上用它的一阶导数去替代它，表示纯度的同时简化计算。

6.

- **预剪枝**：其中的核心思想就是，在每一次实际对结点进行进一步划分之前，先采用验证集的数据来验证如果划分是否能提高划分的准确性。如果不能，就把结点标记为叶结点并退出进一步划分；如果可以就继续递归生成节点。
- **后剪枝**：后剪枝则是先从训练集生成一颗完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来泛化性能提升，则将该子树替换为叶结点。
