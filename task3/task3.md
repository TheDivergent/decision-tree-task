# 第三次打卡练习

练习题

1.(f-E(f)),(E(fD)-fD),σ三个部分表达式的期望都为0.

2.不对。模型拥有大的偏差、大的方差，这种说法是不对的，方差和偏差的大小只能在两个模型对于同一个任务预测上进行比较。两个模型进行比较的话，是可能出现A比B的偏差和方差都要小的情况的，这种情况下，好的模型相比于坏的模型是能够做到的。所谓的”偏差——方差权衡“更多的是指能力相同的模型或者同一模型在不同参数下无法达到偏差和方差同时达到最优的现象，你只能根据实际的任务需求调整自己的模型，就如同模型的准确性和泛化能力无法同时兼顾，精确率和召回率无法同时最优。

3.(100/(1-1/e))/10=15.8轮

4.stacking:

​	训练次数：m*k+1

​	预测次数:2*m*k+1

​	blending:

​	训练次数:m+1

​	预测次数:2*m+1

知识回顾：

1.预测数据的平局损失可以分解为三项，模型的偏差+模型的方差+噪声项。

偏差是指真实模型值（实际上就是数据）与预测模型平均值（你训练得到的模型）的偏差。

方差是指由于训练集的差异，而导致的模型预测值的方差（反映的是输入样本变化导致的预测值变化的波动程度）。

2.相比于单个学习器，通过多个学习器的并行，bagging的方法能够降低方差；通过多个学习器的串行，boosting的方法能够降低偏差。

3.

stacking：通过k折交叉验证的方法，将数据集放入m个模型进行训练，得到m个训练集的结果以及测试集的结果（作为最终模型的输入），将训练集的结果concat，作为m维的特征和真实数据标签作为y输入到最终模型进行训练，最后将m个基学习器的测试集结果，作为最终模型的输入，得到测试的预测标签。

blending：其实流程实现和上面一样，唯一的区别就是未使用交叉验证的方法，而是简单地通过对数据集进行划分的方法，将训练集进一步划分为训练集+验证集，每个基模型训练时用训练集进行训练，验证进行验证，最终模型的训练使用的是m个基模型验证集concat的结果。

这两种方法有一个地方要注意：就是stacking中每个基学习器得到的测试集结果要除以k折交叉验证的个数，

而后者由于没有使用交叉验证，所以不用。

