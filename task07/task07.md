# 练习

1.请写出$L^{(m)}(F_i^{(m)})$​在$F_i^{(m)}=F_i^{(m-1)}$​处的二阶展开。

答：$L^{(m)}(F^{(m)}_i)=\gamma T+\frac{1}{2}\lambda{\sum^T_{j=1}w^2_j}+\sum^N_{i=1}L(y_i,F^{(m)}_i)=\gamma T+\frac{1}{2}\lambda\sum^T_{j=1}w^2_j+\sum^N_{i=1}(L(y_i,F^{(m-1)}_i)+\frac{\part L}{\part{F^{(m)}_i}}|_{F^{(m)}_i=F^{(m-1)}_i}(F^{(m)}_i-F^{(m-1)}_i)+\frac{1}{2}\frac{\part^2L}{\part {F^{(m)}_i}^2}|_{F^{(m)}_i=F^{(m-1)}_i}(F^{(m)}_i-F^{(m-1)}_i)^2)$​​



2.试说明不将损失函数展开至更高阶的原因。

答：实际上对于xgb的损失函数展开到二阶的思想就是用该点的二次函数来对复杂函数进行拟合（降低复杂度，便于计算），求出二次函数的最小点，将两点之间的距离作为每一步更新的步长，所以才会要求该点的二阶导数大于0，这有这种情况下用二次展开来拟合才会有最小值。如果换为更高阶就不具有这个性质了，例如三级展开为(x-1)^3可能就没有最小点，就无法用该思想进行拟合了求更新步长了。

3.请写出平方损失下的近似损失。

答:$L^{(m)}(h^{(m)})=\gamma T+\frac{1}{2}\lambda\sum^T_{j=1}w^2_j+\sum^N_{i=1}(y_i-F^{(m-1)}_i-h^{(m)}(x_i))^2$

$$p_i=-2(y_i-F^{(m-1)}_i-h^{(m)}_i)|_{h_i=0}=-2(y_i-F^{(m-1)}_i)$$

$$q_i=2$$

$$\overline{L}^{(m)}(w^*)=\gamma T-\frac{1}{2}\sum^T_{j=1}\frac{(\sum_{i\in I_j}-2(y_i-F^{(m-1)}_i))^2}{\sum_{i\in I_j}2+\lambda}=\gamma T-2\sum^T_{j=1}\frac{(\sum_{i\in I_j}r_i)^2}{\sum_{i\in I_j}2+\lambda}$$​，其中$r_i=y_i-F^{(m-1)}_i$，为m-1轮训练完之后的误差，$I_j$为决策树每个叶节点样本的集合。

4.在下列的三个损失函数$L(y,\hat{y})$​中，请选出一个不应作为XGBoost损失的函数并说明理由。

- Root Absolute Error: $\sqrt{|y-\hat{y}|}$​​
- Squared Log Error: $\frac{1}{2}[log(\frac{y+1}{\hat{y}+1})]^2$
- Pseudo Huber Error: $\delta^2(\sqrt{1+(\frac{y-\hat{y}}{\delta})^2}-1)$

答：

- 不符合。
- 符合
- 符合。

5.请求出顶点最大度（即最多邻居数量）为d的无向图在最差和最好情况下需要多少种着色数，同时请构造对应的例子。

答：最少需要两种着色，网络图为星型，中间点的度为d，即可。

最多需要d+1种，网络图中的每个点都和其他d个点相连。

6.在最差情况下LightGBM会生成几族互斥特征？这种情况的发生需要满足什么条件？

答：样本特征数有多少族最差就多少族互斥特征。需要满足任意两个特征都不互斥。





# 知识回顾

1.GBDT和梯度下降方法有什么联系？

答：梯度下降法只是在GBDT问题中，求解最小损失函数，所选择的一种优化算法。当然也可以采用其他的优化算法来进行优化，不一定非得是梯度下降法。

2.请叙述GBDT用于分类问题的算法流程。

答：对于K分类问题，我们假设得到了k个得分$F_{1i},...,F_{ki}$​​来代表样本i对应类的可能性大小，对其进行softmax归一化后就可以得到这些类别对应的概率。这样这个分类问题就可以转化为回归问题，用k棵树估计这k类的可能性值F。相比于原来回归问题一个样本只返回一个预测值，无非是这里要预测k个类别的值。在完成这个回归问题的训练后，我们对新样本进行预测，可以得到一个k维向量，输出其值最大的类作为多分类预测的结果即可。

这就是用GBDT实现分类问题的流程。当然在实现分类问题的算法过程还有一些优化技巧，可以将训练K棵树的复杂度降为训练K-1棵树，在K较小的时候，效率会得到很大的提升。​

3.XGBoost和GBDT树有何异同？（可从目标损失、近似方法、分裂依据等方面考虑）

答：

- 从目标损失函数考虑：在使用CART树作为基分类器时，相比于GBDT，XGB目标损失函数再它的基础上显示地加入了限制树生长的正则项，防止树生长地过拟合，从而提高模型地泛化能力。
- 近似方法：在近似计算时，GBDT只是使用了损失函数的一阶导数信息，而XGB则是使用了损失函数的一阶导数和二阶导数信息。
- 分裂依据：GBDT分裂时未考虑树的深度、叶子节点个数的影响，而XGB树在分裂时会考虑这些因素。
- 传统的GDBT采用CART作为基分类器，XGB支持多种类型的基分类器，比如线性分类器。
- GBDT是机器学习算法，而XGBoost是该算法的工程实现。
- 传统的GBDT在每轮迭代时使用全部的数据，XGBoost则采用了与随机森林相似的策略，支持对数据进行采样。
- 传统的GBDT没有设计对缺失值进行处理，XGB能够自动学习出缺失值的处理策略。



4.请叙述LightGBM中GOSS和EFB的作用及算法流程。

答：

GOSS算法流程：在计算信息增益的过程中，对样本梯度绝对值进行排序(降序)，对前a%样本全部取出，而在剩下(1-a%）的样本进行采样b%，这样我们通过采样近似逼近的方法对整个样本进行计算。由于并未计算整体样本数量，而是采用采样的方法，在很大程度上降低了计算量，这也是为什么lgb比xgb快很多，但是却没有降低精确度的原因之一。

EFB算法流程：在lgb要构建直方图的过程中，我们可以通过EFB算法来降低时间复杂度。整体思想就是，将一族互斥特征合并为单个特征（一族互斥特征指的是该族内任意两个特征都不同时取非零值，在任意样本上），这样构建直方图的过程就会快很多。





