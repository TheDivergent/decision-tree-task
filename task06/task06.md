# 练习

1.对于均方损失函数和绝对值损失函数，请分别求出模型的初始预测$F_0$​。

答：$F_0(x_i)=argmin _{\hat{y}}{\sum_{i=1}^NL(y_i,\hat{y})}=argmin_{\hat{y}}\frac{1}{N}{\sum_{i=1}^N(y_i-\hat{y})^2}$​​.可以求得​​​​$F_0=\hat{y}=\frac{1}{N}\sum_{i=1}^N y_i$​.

​	$F_0=argmin _{\hat{y}}{\sum_{i=1}^N{L(y_i,\hat{y})}}=argmin_{\hat{y}}\sum_{i=1}^N|y_i-\hat{y}|$.可以求得$F_0=\hat{y}=median(y).$

2.给定了上一轮的预测结果$F_{m-1}(X_i)$​​和样本标签$y_i$​，请计算使用平方损失时需要拟合的$w_i$​。

答：$L(y,\hat{y})=\frac{1}{2}(y-\hat{y})^2$

$L(w_i)=(r_i-w_i)^2$

$w_i^*=0-\frac{\partial{L}}{\partial{w}}|_{w=0}=r_i$​​​

3.当样本i计算得到的残差ri=0时，本例中的函数在w=0处不可导，请问当前轮应当如何处理模型输出？

答：概论计算残差时得到ri=0时，说明w已经取到了最优值，不用进行更新操作。

4.除了梯度下降法之外，还可以使用[牛顿法](https://en.wikipedia.org/wiki/Newton's_method_in_optimization)来逼近最值点。请叙述基于牛顿法的GBDT回归算法。

答：牛顿法求解最优值的更新公式为$X_{n+1}=X_n-\frac{f^{'}(x_n)}{f^{"}(x_n)}$​​.

​	如果考虑用平方损失函数作为L，则$L(w_i)=(r_i-w_i)^2$

​	可以计算出$w_i^{*}=r_i$.我们可以发现在此种情况下的结果与梯度下降法是一致的。

5.请验证多分类负梯度的结果。

答：

- 未优化的多分类负梯度计算

$$ L(y_i,F_i)=-\sum_{c=1}^Ky_{ci}log\frac{e^{F_{ci}}}{\sum_{c=1}^{K}e^{F_{ci}}}$$

$$ F^{\*(m)}_i=F^{\*(m-1)}_i-\frac{\partial{L}}{\partial{F_i}}|_{F_i=F_i^{(m-1)}}$$

$$-\frac{\partial{L}}{\partial{F_{ki}}}|_{F_i=F_i^{(m-1)}}=\frac{\part}{\part{F_{ki}}}\sum_{c=1}^{K}y_{ci}log\frac{e^{F_{ci}}}{\sum_{c=1}^Ke^{F_{ci}}}|_{F_i=F_i^{(m-1)}} =y_{ki}-\frac{\sum_{c=1}^Ky_{ci}}{\sum_{c=1}^Ke^{F_{ci}}}e^{F_{ki}^{(m-1)}} $$​

其中$\sum_{c=1}^Ky_{ci}=1$,则上式

$$=y_{ki}-\frac{e^{F_{ki}^{(m-1)}}}{\sum_{c=1}^Ke^{F_{ci}^{(m-1)}}}$$

- 优化的多分类负梯度计算

经过概率和为一的优化过后，K棵树变为K-1棵树的拟合，则概率分布变为

$${1+\sum_{c=1}^{K-1}e^{F_{ci}}},\frac{1}{1+\sum_{c=1}^{K-1}e^{F_{ci}}}]$$​

对应的损失函数为

$L(F_{1i},...,F_{(k-1)i})=y_{ki}log(1+\sum_{c=1}^{K-1}e^{F_{ci}})-\sum_{k=1}^{K-1}y_{ki}log\frac{e^{F_{ki}}}{1+\sum_{c=1}^{K-1}e^{F_{ci}}}$

$$-\frac{\part{L}}{\part{F_{ki}}}|_{F_i=F_i^{(m-1)}}=\begin{cases}\frac{e^{F_{ki}^{(m-1)}}}{1+\sum_{c=1}^{K-1}e^{F_{ci}^{(m-1)}}}&y_{ki}=1\\y_{ki}-\frac{e^{F_{ki}^{(m-1)}}}{1+\sum_{c=1}^{K-1}e^{F_{ci}^{(m-1)}}}&y_{ki}=0\end{cases}$$



这里在计算$y_{ki}=0$​条件下的梯度时，可以参考之前未优化部分的计算，会发现结果其实是一样的，只不过是从K维降低为了K-1维。

6.请验证二分类负梯度的结果。

答：当K=2时，$y_i=\{0,1\}$​，此时的可能性向量为$[1,e^{F_i}]$,损失函数写作

$$L(F_i)=-y_ilog\frac{e^{F_i}}{1+e^{f_i}}-(1-y_i)log\frac{1}{1+e^{F_i}}$$

$$-\frac{\part{L}}{\part{F_i}}|_{F_i=F_i^{(m-1)}}=y_i(1-\frac{e^{F_i}}{1+e^{F_i}})+(1-y_i)(-\frac{e^{F_i}}{1+e^{f_i}})=y_i-\frac{e^{F_i}}{1+e^{F_i}}$$​

7.设二分类数据集中正样本比例为10%，请计算模型的初始参数$F^{(0)}$​。

答：对于二分类问题，其初始化样本的的负正样本的概率分布应该为$[\frac{1}{1+e^{F_i^{(0)}}},\frac{e^{F_i^{(0)}}}{1+e^{F_i^{(0)}}}]$​,它的值为[0.9,0.1].

可以求出$F_i^{(0)}=-ln9=-2.197$​

8.请写出$L^{(m)}(F_i^{(m)})$​在$F_i^{(m)}=F_i^{(m-1)}$处的二阶展开。

9.试说明不将损失函数展开至更高阶的原因。

10.请写出平方损失下的近似损失。

11.在下列的三个损失函数$L(y,\hat{y})$​中，请选出一个不应作为XGBoost损失的函数并说明理由。

- Root Absolute Error: $\sqrt{|y-\hat{y}|}$​​
- Squared Log Error: $\frac{1}{2}[log(\frac{y+1}{\hat{y}+1})]^2$
- Pseudo Huber Error: $\delta^2(\sqrt{1+(\frac{y-\hat{y}}{\delta})^2}-1)$

12.请求出顶点最大度（即最多邻居数量）为d的无向图在最差和最好情况下需要多少种着色数，同时请构造对应的例子。

13.在最差情况下LightGBM会生成几族互斥特征？这种情况的发生需要满足什么条件？





# 知识回顾

1.GBDT和梯度下降方法有什么联系？

答：梯度下降法只是在GBDT问题中，求解最小损失函数，所选择的一种优化算法。当然也可以采用其他的优化算法来进行优化，不一定非得是梯度下降法。

2.请叙述GBDT用于分类问题的算法流程。

答：对于K分类问题，我们假设得到了k个得分$F_{1i},...,F_{ki}$​​来代表样本i对应类的可能性大小，对其进行softmax归一化后就可以得到这些类别对应的概率。这样这个分类问题就可以转化为回归问题，用k棵树估计这k类的可能性值F。相比于原来回归问题一个样本只返回一个预测值，无非是这里要预测k个类别的值。在完成这个回归问题的训练后，我们对新样本进行预测，可以得到一个k维向量，输出其值最大的类作为多分类预测的结果即可。

这就是用GBDT实现分类问题的流程。当然在实现分类问题的算法过程还有一些优化技巧，可以将训练K棵树的复杂度降为训练K-1棵树，在K较小的时候，效率会得到很大的提升。​

3.XGBoost和GBDT树有何异同？（可从目标损失、近似方法、分裂依据等方面考虑）

4.请叙述LightGBM中GOSS和EFB的作用及算法流程。



